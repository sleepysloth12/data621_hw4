---
title: "Data 621 HW 4"
author: 'Group #3'
date: "2024-04-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Insurance Data Analysis

### Packages Used

```{r}
library(tidyverse)
library(summarytools)
library(corrplot)
library(caret)
library(MASS)
```

### Importing Datasets

```{r}

insur_train_raw=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/data621_hw4/main/insurance_training_data.csv"))

insur_test=read.csv(url("https://github.com/sleepysloth12/data621_hw4/raw/main/insurance-evaluation-data.csv"))
```

## Part 1 - Data Exploration

```{r}

num_insur_dat = select_if(insur_train_raw, is.numeric)

train_stats = dfSummary(num_insur_dat, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats)
```

```{r}
cor_matrix = cor(num_insur_dat, use = "complete.obs")

corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black")
```

### **Weak Correlations:**

-   **`INDEX` with any variable:** The correlations are very close to 0, meaning no linear relationship. We will remove this

-   **`TRAVTIME` and most variables:** Most correlations with **`TRAVTIME`** (distance to work) are very weak, indicating that the distance to work does not have a strong linear relationship with these variables. We will remove this

-   **Correlations involving `TIF`, `YOJ`, `CAR_AGE` with other variables:** These are weak, indicating limited linear relationships. For example, **`TIF`** (time in force) and **`YOJ`** (years on job) don't strongly correlate with risk indicators like **`CLM_FREQ`** or **`MVR_PTS`**, meaning loyalty or job stability isn't strongly associated with driving risk in a linear manner.

### **Significant Correlations:**

-   **`TARGET_FLAG` and `TARGET_AMT` :** This makes sense as all cars involved in a crash would be charged.

-   **`TARGET_FLAG` and `CLM_FREQ` , `MVR_PTS`:** These moderate correlations suggest that the likelihood of being in a crash is positively associated with the number of claims filed in the past 5 years (**`CLM_FREQ`**) and the motor vehicle record points (**`MVR_PTS`**). These variables could be risky driving behavior that leads to accidents.

-   **`KIDSDRIV` and `HOMEKIDS` :** A moderate to strong positive correlation indicates that the number of driving children (**`KIDSDRIV`**) tends to be higher in homes with more children (**`HOMEKIDS`**). This relationship is expected.

-   **`AGE` and `HOMEKIDS` :** This negative correlation suggests that younger drivers tend to have more children at home.

-   **`AGE` and `CAR_AGE` :** A weak positive correlation indicates older drivers tend to have older cars.

-   **`CLM_FREQ` and `MVR_PTS` :** A moderate positive correlation suggests that drivers with more claims also tend to have more motor vehicle record points. Both are indicators of risky driving behavior.

## Part 2 - Data Preparation

### Removing Weak Correlations

First, I will remove the variables flagged above for weak correlations. These will be `index` , **`TRAVTIME` , `TIF`** , **`YOJ`** , **`CAR_AGE`**

```{r}

insur_train_1= insur_train_raw %>%
  select(-c(INDEX, TRAVTIME, TIF, YOJ, CAR_AGE))
```

### Dealing with missing Data

Now I will look for missing data.

```{r}
missing_data_report = insur_train_1 %>%
  summarise_all(~sum(is.na(.)))

print(missing_data_report)
```

After removing those columns that had weak correlations, the only variable that had missing data was the AGE variable, with only 0.1% of missing data (6 missing out of 8155). Since the sample size is large, the amount of missing data is low relative to the sample size, and the distribution of most of the age data is normal, we can use mean imputation to fill in for these missing values.

```{r}
mean_age = mean(insur_train_1$AGE, na.rm = TRUE)

insur_train_1$AGE[is.na(insur_train_1$AGE)] = mean_age

sum(is.na(insur_train_1$AGE))

```

Now, there is no more missing data.

### Creating New Variables

Now, I will create new variables.

**`CLM_FREQ` and `MVR_PTS`** had correlation between them. We will use PCA to combine **`CLM_FREQ`** and **`MVR_PTS`** into a single component. This approach is particularly useful if wewant to capture the most variance from both variables in a single composite variable.

```{r}
risk_variables = data.frame(CLM_FREQ = insur_train_1$CLM_FREQ, MVR_PTS = insur_train_1$MVR_PTS)


pca_result = prcomp(risk_variables, center = TRUE, scale. = TRUE)
insur_train_1$PCA_Score = pca_result$x[,1]

insur_train_2 = insur_train_1 %>%
  select(-c(CLM_FREQ, MVR_PTS))
```

Given the moderate to strong positive correlation between **`KIDSDRIV`** (the number of driving children) and **`HOMEKIDS`** (the number of children at home), I will calculate proportion of children who drive relative to the total number of children in the household.

```{r}
insur_train_2 = insur_train_2 %>%
  mutate(HOMEKIDS = ifelse(HOMEKIDS == 0, NA, HOMEKIDS),
         Driving_Ratio = KIDSDRIV / HOMEKIDS)%>%
  select(-c(HOMEKIDS, KIDSDRIV))
```

### Binning Categorical Variables

There are many categorical variables.

I will start binning them and creating dummy variables.

`is_urban` -1 if urban 0 if else

`is_revoked` - 1 if Yes 0 if else

`is_red_car` - 1 if yes 0 if else

`is_single_parent` - 1 if yes 0 if else

`is_married` - 1 if yes 0 if else

`is_male` - 1 if yes 0 if else

`is_fam_car` - 1 if minivan, suv, 0 if else

`is_higher_ed`  - 1 if bachelors or above 0 if else

`is_commercial` - 1 if commercial 0 if else

`is_profesional` - 1 if professional, doctor, lawyer, manager, 0 if else

```{r}
#str(insur_train_13)

#unique(insur_train_2$URBANICITY)

insur_train_3 = insur_train_2 %>%
  mutate(is_urban=ifelse(URBANICITY=="Highly Urban/ Urban", 1 , 0))%>%
  select(-URBANICITY)

#unique(insur_train_2$REVOKED)

insur_train_4 = insur_train_3 %>%
  mutate(is_revoked=ifelse(REVOKED=="Yes", 1 , 0))%>%
  select(-REVOKED)

insur_train_5 = insur_train_4 %>%
  mutate(is_red_car=ifelse(RED_CAR=="yes", 1 , 0))%>%
  select(-RED_CAR)

insur_train_6 = insur_train_5 %>%
  mutate(is_single_parent=ifelse(PARENT1=="Yes", 1 , 0))%>%
  select(-PARENT1)

#unique(insur_train_6$MSTATUS)

insur_train_7 = insur_train_6 %>%
  mutate(is_married=ifelse(MSTATUS=="Yes", 1 , 0))%>%
  select(-MSTATUS)

#unique(insur_train_6$SEX)
insur_train_8 = insur_train_7 %>%
  mutate(is_married=ifelse(SEX=="M", 1 , 0))%>%
  select(-SEX)

#unique(insur_train_6$CAR_TYPE)

insur_train_9 = insur_train_8 %>%
  mutate(is_fam_car = ifelse(CAR_TYPE %in% c("Minivan", "z_SUV"), 1, 0)) %>%
  select(-CAR_TYPE) 

#unique(insur_train_6$EDUCATION)

insur_train_10 = insur_train_9 %>%
  mutate(is_higher_Ed = ifelse(EDUCATION %in% c("PhD", "Bachelors","Masters"), 1, 0)) %>%
  select(-EDUCATION) 

insur_train_11 = insur_train_10 %>%
  mutate(is_commercial=ifelse(CAR_USE=="Commercial", 1 , 0))%>%
  select(-CAR_USE)

#unique(insur_train_6$JOB)

insur_train_12 = insur_train_11 %>%
  mutate(is_professional = ifelse(JOB %in% c("Professional", "Manager","Doctor","Lawyer"), 1, 0)) %>%
  select(-JOB) 
```

`Driving_Ratio` variable has a lot of missing values, it does not make sense really. I will remove it.

```{r}
insur_train_13 = insur_train_12 %>%
  select(-Driving_Ratio)
```

###  Processing cost data

All cost variables like `INCOME`, `HOME_VAL`, and `BLUEBOOK` are in character form and need ro be processed to numeric.

```{r}
insur_train_14 = insur_train_13 %>%
  mutate(
    INCOME = as.numeric(gsub("[\\$,]", "", INCOME)),
    HOME_VAL = as.numeric(gsub("[\\$,]", "", HOME_VAL)),
    BLUEBOOK = as.numeric(gsub("[\\$,]", "", BLUEBOOK)),
    OLDCLAIM = as.numeric(gsub("[\\$,]", "", OLDCLAIM))
  )

str(insur_train_14)
```

### Reanalyzing the Distribution

Now that all of our variable are numerical, let us reanalyze their distribution and see whether or not there is missing data in each.

```{r}
train_stats_2 = dfSummary(insur_train_14, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats_2)
```

### Median and Mean Imputation

There are 445 missing values for `income` and 464 missing values for `home_Val`.

I will do median imputation to fill in for these missing values. Median imputation is a more accurate measure than mean imputation for income. For home value, Mean is more accurate (because there is a large concentration of non-homeowners).

```{r}
median_income = median(insur_train_14$INCOME, na.rm = TRUE)

mean_home_val = mean(insur_train_14$HOME_VAL, na.rm = TRUE)

insur_train_15 = insur_train_14 %>%
  mutate(
    INCOME = ifelse(is.na(INCOME), median_income, INCOME),
    HOME_VAL = ifelse(is.na(HOME_VAL), median_home_val, HOME_VAL)
  )
```

###  Log Transform

Due to the wide range and skewedness of some columns, I preformed a Log transform. I also added by 1 to avoid errors with 0.

```{r}
insur_train_16 = insur_train_15 %>%
  mutate(
    log_INCOME = log(INCOME + 1),
    log_HOME_VAL = log(HOME_VAL + 1),
    log_BLUEBOOK = log(BLUEBOOK + 1),
    log_OLDCLAIM = log(OLDCLAIM + 1),
    log_PCA_Score=log(PCA_Score + 1)
  )%>%
  select(-c(INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM, PCA_Score))

train_stats_3 = dfSummary(insur_train_16, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats_3)
```

### Processing Train Dataset same way as test

```{r}

# Removing Weak Correlations
insur_test_1 = insur_test %>%
  dplyr::select(-c(INDEX, TRAVTIME, TIF, YOJ, CAR_AGE))


mean_age = mean(insur_test_1$AGE, na.rm = TRUE)
insur_test_1$AGE[is.na(insur_test_1$AGE)] = mean_age


risk_variables_test = data.frame(CLM_FREQ = insur_test_1$CLM_FREQ, MVR_PTS = insur_test_1$MVR_PTS)
pca_result_test = prcomp(risk_variables_test, center = TRUE, scale. = TRUE)
insur_test_1$PCA_Score = pca_result_test$x[,1]
insur_test_2 = insur_test_1 %>%
  dplyr::select(-c(CLM_FREQ, MVR_PTS))


insur_test_3 = insur_test_2 %>%
  mutate(is_urban = ifelse(URBANICITY == "Highly Urban/ Urban", 1, 0)) %>%
  mutate(is_revoked = ifelse(REVOKED == "Yes", 1, 0)) %>%
  mutate(is_red_car = ifelse(RED_CAR == "yes", 1, 0)) %>%
  mutate(is_single_parent = ifelse(PARENT1 == "Yes", 1, 0)) %>%
  mutate(is_married = ifelse(MSTATUS == "Yes", 1, 0)) %>%
  mutate(is_male = ifelse(SEX == "M", 1, 0)) %>%
  mutate(is_fam_car = ifelse(CAR_TYPE %in% c("Minivan", "z_SUV"), 1, 0)) %>%
  mutate(is_higher_Ed = ifelse(EDUCATION %in% c("PhD", "Bachelors", "Masters"), 1, 0)) %>%
  mutate(is_commercial = ifelse(CAR_USE == "Commercial", 1, 0)) %>%
  mutate(is_professional = ifelse(JOB %in% c("Professional", "Manager", "Doctor", "Lawyer"), 1, 0)) %>%
  dplyr::select(-c(URBANICITY, REVOKED, RED_CAR, PARENT1, MSTATUS, SEX, CAR_TYPE, EDUCATION, CAR_USE, JOB))

# Processing Cost Data: Converting to Numeric and Imputing Missing Values
insur_test_14 = insur_test_3 %>%
  mutate(
    INCOME = as.numeric(gsub("[\\$,]", "", INCOME)),
    HOME_VAL = as.numeric(gsub("[\\$,]", "", HOME_VAL)),
    BLUEBOOK = as.numeric(gsub("[\\$,]", "", BLUEBOOK)),
    OLDCLAIM = as.numeric(gsub("[\\$,]", "", OLDCLAIM))
  )

# Imputation for Missing Values in INCOME and HOME_VAL
median_income = median(insur_test_14$INCOME, na.rm = TRUE)
median_home_val = median(insur_test_14$HOME_VAL, na.rm = TRUE)

insur_test_15 = insur_test_14 %>%
  mutate(
    INCOME = ifelse(is.na(INCOME), median_income, INCOME),
    HOME_VAL = ifelse(is.na(HOME_VAL), median_home_val, HOME_VAL)
  )

# Log Transform
insur_test_16 = insur_test_15 %>%
  mutate(
    log_INCOME = log(INCOME + 1),
    log_HOME_VAL = log(HOME_VAL + 1),
    log_BLUEBOOK = log(BLUEBOOK + 1),
    log_OLDCLAIM = log(OLDCLAIM + 1)
  ) %>%
  dplyr::select(-c(INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM))



```

```{r}
insur_train_clean=insur_train_16

insur_test_clean=insur_test_16
```

## Part 3 - Build Models

### Splitting the Dataframes

```{r}
insur_train_clean_lin = insur_train_clean %>%
  dplyr::select(-TARGET_FLAG)
  
insur_train_clean_bin = insur_train_clean %>%
  dplyr::select(-TARGET_AMT)
```

### Model 1: Multiple Linear Regression Model One

```{r}
base_model_1 = lm(TARGET_AMT ~ ., insur_train_clean_lin)

step_backward = ols_step_backward_p(base_model)

lin_model_1 = step_backward$model

summary(model_1)
```

Here, we fit an initial linear regression model with all available predictors and then perform step-wise backward selection to remove predictors with non-significant p-values. This process helps in identifying a parsimonious model which only retains significant predictors for predicting the response variable based on p-value.

This model does have an overall statistically significant p-value, but the $R^2$ value is less than desirable.

### Model 2: Multiple Linear Regression Model Two

```{r}
base_model_2 <- lm(TARGET_AMT ~ ., data = insur_train_clean_lin)

step_forward = ols_step_forward_p(base_model_2, 0.1, hierarchical = TRUE)

lin_model_2 = step_forward$model

summary(model_2)
```

Here, we fit an initial linear regression model with all available predictors. Again, we perform stepwise selection (this time forward) to add predictors with significant p-values. The key detail here is that we specified a significance level of 0.1 for adding predictors to the model and we set hierarchical = TRUE, which means that the step-wise procedure should consider the hierarchical relationships between predictors. (essentially, we consider the interdependence between predictor variables and avoid the inclusion of redundant or collinear predictors).

Again, this model does have an overall statistically significant p-value, but the $R^2$ value is less than desirable.

### Model 3: Binary Logistic Regression Model One

```{r}
base_bin_model <- glm(TARGET_FLAG ~ ., data = insur_train_clean_bin, family = binomial)

step_aic = stepAIC(base_bin_model, direction = 'both')

bin_model_1 = glm(formula = TARGET_FLAG ~ is_urban + is_revoked + is_single_parent + 
    is_married + is_fam_car + is_higher_Ed + is_commercial + 
    is_professional + log_INCOME + log_HOME_VAL + log_BLUEBOOK + 
    log_OLDCLAIM + log_PCA_Score, family = binomial, data = insur_train_clean_bin)
```

For Model 3, I have also used a step-wise variable selection process, though this time the selection was conducted using the stepAIC function (steps are informed using the model's AIC value). I specified that steps can be taken in both directions.

### Model 4: Binary Logistic Regression Model Two

```{r}
step(base_bin_model, 
                scope = list(lower = TARGET_FLAG ~ 1, 
                             upper = TARGET_FLAG ~ .),
               direction = "both", k = log(nrow(na.omit(insur_train_clean_bin))))

bin_model_2 = glm(formula = TARGET_FLAG ~ is_urban + is_revoked + is_single_parent + 
    is_married + is_fam_car + is_higher_Ed + is_commercial + 
    is_professional + log_INCOME + log_HOME_VAL + log_BLUEBOOK + 
    log_PCA_Score, family = binomial, data = insur_train_clean_bin)
```

Now, considering that AIC step-wise variable selection has the tendency to include too many variables, for my next variable selection process, I opted for a BIC step wise variable selection approach. The AIC values associated with Model 3 and this model (Model 4) are rather comparable, meaning they have relatively similar goodness of fit as defined by AIC.

### Model 5: Binary Logistic Regression Model Three

```{r}
missing_values <- colnames(insur_train_clean_bin)[apply(insur_train_clean_bin, 2, anyNA)]
if (length(missing_values) > 0) {
  cat("The following columns have missing values:", paste(missing_values, collapse = ", "), "\n")
  insur_train_clean_bin <- na.omit(insur_train_clean_bin)
}

X <- as.matrix(insur_train_clean_bin[, -which(names(insur_train_clean_bin) == "TARGET_FLAG")])
y <- insur_train_clean_bin$TARGET_FLAG

cv_model <- cv.glmnet(X, y, family = "binomial", alpha = 1)

plot(cv_model)

optimal_lambda <- cv_model$lambda.min
print(paste("Optimal lambda:", optimal_lambda))

final_model <- glmnet(X, y, family = "binomial", alpha = 1, lambda = optimal_lambda)

coef(final_model)
```

For the final model selection, I conducted a Lasso Regression.

## Part 4 - Select Models
