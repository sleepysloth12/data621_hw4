---
title: "Data 621 HW 4"
author: 'Group #3'
date: "2024-04-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Insurance Data Analysis

### Packages Used

```{r}
library(tidyverse)
library(summarytools)
library(corrplot)
library(caret)
library(MASS)
library(olsrr)
library(glmnet)
library(glmnetUtils)
library(flexmix)
library(knitr)
library(pROC)
library(glue)
```

### Importing Datasets

```{r}

insur_train_raw=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/data621_hw4/main/insurance_training_data.csv"))

insur_test=read.csv(url("https://github.com/sleepysloth12/data621_hw4/raw/main/insurance-evaluation-data.csv"))
```

## Part 1 - Data Exploration

```{r}

num_insur_dat = select_if(insur_train_raw, is.numeric)

train_stats = dfSummary(num_insur_dat, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats)
```

```{r}
cor_matrix = cor(num_insur_dat, use = "complete.obs")

corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, 
         addCoef.col = "black")
```

### **Weak Correlations:**

-   **`INDEX` with any variable:** The correlations are very close to 0, meaning no linear relationship. We will remove this

-   **`TRAVTIME` and most variables:** Most correlations with **`TRAVTIME`** (distance to work) are very weak, indicating that the distance to work does not have a strong linear relationship with these variables. We will remove this

-   **Correlations involving `TIF`, `YOJ`, `CAR_AGE` with other variables:** These are weak, indicating limited linear relationships. For example, **`TIF`** (time in force) and **`YOJ`** (years on job) don't strongly correlate with risk indicators like **`CLM_FREQ`** or **`MVR_PTS`**, meaning loyalty or job stability isn't strongly associated with driving risk in a linear manner.

### **Significant Correlations:**

-   **`TARGET_FLAG` and `TARGET_AMT` :** This makes sense as all cars involved in a crash would be charged.

-   **`TARGET_FLAG` and `CLM_FREQ` , `MVR_PTS`:** These moderate correlations suggest that the likelihood of being in a crash is positively associated with the number of claims filed in the past 5 years (**`CLM_FREQ`**) and the motor vehicle record points (**`MVR_PTS`**). These variables could be risky driving behavior that leads to accidents.

-   **`KIDSDRIV` and `HOMEKIDS` :** A moderate to strong positive correlation indicates that the number of driving children (**`KIDSDRIV`**) tends to be higher in homes with more children (**`HOMEKIDS`**). This relationship is expected.

-   **`AGE` and `HOMEKIDS` :** This negative correlation suggests that younger drivers tend to have more children at home.

-   **`AGE` and `CAR_AGE` :** A weak positive correlation indicates older drivers tend to have older cars.

-   **`CLM_FREQ` and `MVR_PTS` :** A moderate positive correlation suggests that drivers with more claims also tend to have more motor vehicle record points. Both are indicators of risky driving behavior.

## Part 2 - Data Preparation

### Removing Weak Correlations

First, I will remove the variables flagged above for weak correlations. These will be `index` , **`TRAVTIME` , `TIF`** , **`YOJ`** , **`CAR_AGE`**

```{r}

insur_train_1= insur_train_raw %>%
  dplyr::select(-c(INDEX, TRAVTIME, TIF, YOJ, CAR_AGE))
```

### Dealing with missing Data

Now I will look for missing data.

```{r}
missing_data_report = insur_train_1 %>%
  summarise_all(~sum(is.na(.)))

print(missing_data_report)
```

After removing those columns that had weak correlations, the only variable that had missing data was the AGE variable, with only 0.1% of missing data (6 missing out of 8155). Since the sample size is large, the amount of missing data is low relative to the sample size, and the distribution of most of the age data is normal, we can use mean imputation to fill in for these missing values.

```{r}
mean_age = mean(insur_train_1$AGE, na.rm = TRUE)

insur_train_1$AGE[is.na(insur_train_1$AGE)] = mean_age

sum(is.na(insur_train_1$AGE))

```

Now, there is no more missing data.

### Creating New Variables

Now, I will create new variables.

**`CLM_FREQ` and `MVR_PTS`** had correlation between them. We will use PCA to combine **`CLM_FREQ`** and **`MVR_PTS`** into a single component. This approach is particularly useful if wewant to capture the most variance from both variables in a single composite variable.

```{r}
risk_variables = data.frame(CLM_FREQ = insur_train_1$CLM_FREQ, MVR_PTS = insur_train_1$MVR_PTS)


pca_result = prcomp(risk_variables, center = TRUE, scale. = TRUE)
insur_train_1$PCA_Score = pca_result$x[,1]

insur_train_2 = insur_train_1 %>%
  dplyr::select(-c(CLM_FREQ, MVR_PTS))
```

Given the moderate to strong positive correlation between **`KIDSDRIV`** (the number of driving children) and **`HOMEKIDS`** (the number of children at home), I will calculate proportion of children who drive relative to the total number of children in the household.

```{r}
insur_train_2 = insur_train_2 %>%
  mutate(HOMEKIDS = ifelse(HOMEKIDS == 0, NA, HOMEKIDS),
         Driving_Ratio = KIDSDRIV / HOMEKIDS)%>%
  dplyr::select(-c(HOMEKIDS, KIDSDRIV))
```

### Binning Categorical Variables

There are many categorical variables.

I will start binning them and creating dummy variables.

`is_urban` -1 if urban 0 if else

`is_revoked` - 1 if Yes 0 if else

`is_red_car` - 1 if yes 0 if else

`is_single_parent` - 1 if yes 0 if else

`is_married` - 1 if yes 0 if else

`is_male` - 1 if yes 0 if else

`is_fam_car` - 1 if minivan, suv, 0 if else

`is_higher_ed`  - 1 if bachelors or above 0 if else

`is_commercial` - 1 if commercial 0 if else

`is_profesional` - 1 if professional, doctor, lawyer, manager, 0 if else

```{r}
#str(insur_train_13)

#unique(insur_train_2$URBANICITY)

insur_train_3 = insur_train_2 %>%
  mutate(is_urban=ifelse(URBANICITY=="Highly Urban/ Urban", 1 , 0))%>%
  dplyr::select(-URBANICITY)

#unique(insur_train_2$REVOKED)

insur_train_4 = insur_train_3 %>%
  mutate(is_revoked=ifelse(REVOKED=="Yes", 1 , 0))%>%
  dplyr::select(-REVOKED)

insur_train_5 = insur_train_4 %>%
  mutate(is_red_car=ifelse(RED_CAR=="yes", 1 , 0))%>%
  dplyr::select(-RED_CAR)

insur_train_6 = insur_train_5 %>%
  mutate(is_single_parent=ifelse(PARENT1=="Yes", 1 , 0))%>%
  dplyr::select(-PARENT1)

#unique(insur_train_6$MSTATUS)

insur_train_7 = insur_train_6 %>%
  mutate(is_married=ifelse(MSTATUS=="Yes", 1 , 0))%>%
  dplyr::select(-MSTATUS)

#unique(insur_train_6$SEX)
insur_train_8 = insur_train_7 %>%
  mutate(is_married=ifelse(SEX=="M", 1 , 0))%>%
  dplyr::select(-SEX)

#unique(insur_train_6$CAR_TYPE)

insur_train_9 = insur_train_8 %>%
  mutate(is_fam_car = ifelse(CAR_TYPE %in% c("Minivan", "z_SUV"), 1, 0)) %>%
  dplyr::select(-CAR_TYPE) 

#unique(insur_train_6$EDUCATION)

insur_train_10 = insur_train_9 %>%
  mutate(is_higher_Ed = ifelse(EDUCATION %in% c("PhD", "Bachelors","Masters"), 1, 0)) %>%
  dplyr::select(-EDUCATION) 

insur_train_11 = insur_train_10 %>%
  mutate(is_commercial=ifelse(CAR_USE=="Commercial", 1 , 0))%>%
  dplyr::select(-CAR_USE)

#unique(insur_train_6$JOB)

insur_train_12 = insur_train_11 %>%
  mutate(is_professional = ifelse(JOB %in% c("Professional", "Manager","Doctor","Lawyer"), 1, 0)) %>%
  dplyr::select(-JOB) 
```

`Driving_Ratio` variable has a lot of missing values, it does not make sense really. I will remove it.

```{r}
insur_train_13 = insur_train_12 %>%
  dplyr::select(-Driving_Ratio)
```

###  Processing cost data

All cost variables like `INCOME`, `HOME_VAL`, and `BLUEBOOK` are in character form and need ro be processed to numeric.

```{r}
insur_train_14 = insur_train_13 %>%
  mutate(
    INCOME = as.numeric(gsub("[\\$,]", "", INCOME)),
    HOME_VAL = as.numeric(gsub("[\\$,]", "", HOME_VAL)),
    BLUEBOOK = as.numeric(gsub("[\\$,]", "", BLUEBOOK)),
    OLDCLAIM = as.numeric(gsub("[\\$,]", "", OLDCLAIM))
  )

str(insur_train_14)
```

### Reanalyzing the Distribution

Now that all of our variable are numerical, let us reanalyze their distribution and see whether or not there is missing data in each.

```{r}
train_stats_2 = dfSummary(insur_train_14, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats_2)
```

### Median and Mean Imputation

There are 445 missing values for `income` and 464 missing values for `home_Val`.

I will do median imputation to fill in for these missing values. Median imputation is a more accurate measure than mean imputation for income. For home value, Mean is more accurate (because there is a large concentration of non-homeowners).

```{r}
median_income = median(insur_train_14$INCOME, na.rm = TRUE)

mean_home_val = mean(insur_train_14$HOME_VAL, na.rm = TRUE)

insur_train_15 = insur_train_14 %>%
  mutate(
    INCOME = ifelse(is.na(INCOME), median_income, INCOME),
    HOME_VAL = ifelse(is.na(HOME_VAL), mean_home_val, HOME_VAL)
  )
```

###  Log Transform

Due to the wide range and skewedness of some columns, I preformed a Log transform. I also added by 1 to avoid errors with 0.

```{r}
##Adding this to avoid -Inf or NaN values
log_no_neg <- function(x)
{
  max_val <- max(x)
  min_val <- min(x)
  shift <- 0
  if (min_val <= 0)
  {
    shift <- abs(min_val) + 1
  }
  log(x + shift)
}

insur_train_16 = insur_train_15 %>%
  mutate(
    log_INCOME = log_no_neg(INCOME),
    log_HOME_VAL = log_no_neg(HOME_VAL),
    log_BLUEBOOK = log_no_neg(BLUEBOOK),
    log_OLDCLAIM = log_no_neg(OLDCLAIM),
    log_PCA_Score=log_no_neg(PCA_Score)
  )%>%
  dplyr::select(-c(INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM, PCA_Score))

train_stats_3 = dfSummary(insur_train_16, stats = c("mean", "sd", "med", "IQR", "min", "max", "valid", "n.missing"))

view(train_stats_3)
```

### Processing Train Dataset same way as test

```{r}

# Removing Weak Correlations
insur_test_1 = insur_test %>%
  dplyr::select(-c(INDEX, TRAVTIME, TIF, YOJ, CAR_AGE))


mean_age = mean(insur_test_1$AGE, na.rm = TRUE)
insur_test_1$AGE[is.na(insur_test_1$AGE)] = mean_age


risk_variables_test = data.frame(CLM_FREQ = insur_test_1$CLM_FREQ, MVR_PTS = insur_test_1$MVR_PTS)
pca_result_test = prcomp(risk_variables_test, center = TRUE, scale. = TRUE)
insur_test_1$PCA_Score = pca_result_test$x[,1]
insur_test_2 = insur_test_1 %>%
  dplyr::select(-c(CLM_FREQ, MVR_PTS))


insur_test_3 = insur_test_2 %>%
  mutate(is_urban = ifelse(URBANICITY == "Highly Urban/ Urban", 1, 0)) %>%
  mutate(is_revoked = ifelse(REVOKED == "Yes", 1, 0)) %>%
  mutate(is_red_car = ifelse(RED_CAR == "yes", 1, 0)) %>%
  mutate(is_single_parent = ifelse(PARENT1 == "Yes", 1, 0)) %>%
  mutate(is_married = ifelse(MSTATUS == "Yes", 1, 0)) %>%
  mutate(is_male = ifelse(SEX == "M", 1, 0)) %>%
  mutate(is_fam_car = ifelse(CAR_TYPE %in% c("Minivan", "z_SUV"), 1, 0)) %>%
  mutate(is_higher_Ed = ifelse(EDUCATION %in% c("PhD", "Bachelors", "Masters"), 1, 0)) %>%
  mutate(is_commercial = ifelse(CAR_USE == "Commercial", 1, 0)) %>%
  mutate(is_professional = ifelse(JOB %in% c("Professional", "Manager", "Doctor", "Lawyer"), 1, 0)) %>%
  dplyr::select(-c(URBANICITY, REVOKED, RED_CAR, PARENT1, MSTATUS, SEX, CAR_TYPE, EDUCATION, CAR_USE, JOB))

# Processing Cost Data: Converting to Numeric and Imputing Missing Values
insur_test_14 = insur_test_3 %>%
  mutate(
    INCOME = as.numeric(gsub("[\\$,]", "", INCOME)),
    HOME_VAL = as.numeric(gsub("[\\$,]", "", HOME_VAL)),
    BLUEBOOK = as.numeric(gsub("[\\$,]", "", BLUEBOOK)),
    OLDCLAIM = as.numeric(gsub("[\\$,]", "", OLDCLAIM))
  )

# Imputation for Missing Values in INCOME and HOME_VAL
median_income = median(insur_test_14$INCOME, na.rm = TRUE)
median_home_val = median(insur_test_14$HOME_VAL, na.rm = TRUE)

insur_test_15 = insur_test_14 %>%
  mutate(
    INCOME = ifelse(is.na(INCOME), median_income, INCOME),
    HOME_VAL = ifelse(is.na(HOME_VAL), median_home_val, HOME_VAL)
  )

# Log Transform
insur_test_16 = insur_test_15 %>%
  mutate(
    log_INCOME = log_no_neg(INCOME),
    log_HOME_VAL = log_no_neg(HOME_VAL),
    log_BLUEBOOK = log_no_neg(BLUEBOOK),
    log_OLDCLAIM = log_no_neg(OLDCLAIM),
    log_PCA_Score=log_no_neg(PCA_Score)
  )%>%
  dplyr::select(-c(INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM, PCA_Score))



```

```{r}
insur_train_clean=insur_train_16

insur_test_clean=insur_test_16
```

## Part 3 - Build Models

### Splitting the Dataframes

```{r}
insur_train_clean_lin = insur_train_clean %>%
  dplyr::select(-TARGET_FLAG)
  
insur_train_clean_bin = insur_train_clean %>%
  dplyr::select(-TARGET_AMT)
```

### Model 1: Multiple Linear Regression Model One

```{r}
base_model_1 = lm(TARGET_AMT ~ ., insur_train_clean_lin)

step_backward = ols_step_backward_p(base_model_1)

lin_model_1 = step_backward$model

summary(lin_model_1)
```

Here, we fit an initial linear regression model with all available predictors and then perform step-wise backward selection to remove predictors with non-significant p-values. This process helps in identifying a parsimonious model which only retains significant predictors for predicting the response variable based on p-value.

This model does have an overall statistically significant p-value, but the $R^2$ value is less than desirable.

### Model 2: Multiple Linear Regression Model Two

```{r}
base_model_2 <- lm(TARGET_AMT ~ ., data = insur_train_clean_lin)

step_forward = ols_step_forward_p(base_model_2, 0.1, hierarchical = TRUE)

lin_model_2 = step_forward$model

summary(lin_model_2)
```

Here, we fit an initial linear regression model with all available predictors. Again, we perform stepwise selection (this time forward) to add predictors with significant p-values. The key detail here is that we specified a significance level of 0.1 for adding predictors to the model and we set hierarchical = TRUE, which means that the step-wise procedure should consider the hierarchical relationships between predictors. (essentially, we consider the interdependence between predictor variables and avoid the inclusion of redundant or collinear predictors).

Again, this model does have an overall statistically significant p-value, but the $R^2$ value is less than desirable.

### Model 3: Binary Logistic Regression Model One

```{r}
base_bin_model <- glm(TARGET_FLAG ~ ., data = insur_train_clean_bin, family = binomial)

step_aic = stepAIC(base_bin_model, direction = 'both')

bin_model_1 = glm(formula = TARGET_FLAG ~ is_urban + is_revoked + is_single_parent + 
    is_married + is_fam_car + is_higher_Ed + is_commercial + 
    is_professional + log_INCOME + log_HOME_VAL + log_BLUEBOOK + 
    log_OLDCLAIM + log_PCA_Score, family = binomial, data = insur_train_clean_bin)

summary(bin_model_1)
```

For Model 3, I have also used a step-wise variable selection process, though this time the selection was conducted using the stepAIC function (steps are informed using the model's AIC value). I specified that steps can be taken in both directions.

### Model 4: Binary Logistic Regression Model Two

```{r}
step(base_bin_model, 
                scope = list(lower = TARGET_FLAG ~ 1, 
                             upper = TARGET_FLAG ~ .),
               direction = "both", k = log(nrow(na.omit(insur_train_clean_bin))))

bin_model_2 = glm(formula = TARGET_FLAG ~ is_urban + is_revoked + is_single_parent + 
    is_married + is_fam_car + is_higher_Ed + is_commercial + 
    is_professional + log_INCOME + log_HOME_VAL + log_BLUEBOOK + 
    log_PCA_Score, family = binomial, data = insur_train_clean_bin)

summary(bin_model_2)
```

Now, considering that AIC step-wise variable selection has the tendency to include too many variables, for my next variable selection process, I opted for a BIC step wise variable selection approach. The AIC values associated with Model 3 and this model (Model 4) are rather comparable, meaning they have relatively similar goodness of fit as defined by AIC.

### Model 5: Binary Logistic Regression Model Three

```{r}
missing_values <- colnames(insur_train_clean_bin)[apply(insur_train_clean_bin, 2, anyNA)]
if (length(missing_values) > 0) {
  cat("The following columns have missing values:", paste(missing_values, collapse = ", "), "\n")
  insur_train_clean_bin <- na.omit(insur_train_clean_bin)
}

X <- as.matrix(insur_train_clean_bin[, -which(names(insur_train_clean_bin) == "TARGET_FLAG")])
y <- insur_train_clean_bin$TARGET_FLAG

cv_model <- cv.glmnet(X, y, family = "binomial", alpha = 1)

plot(cv_model)

optimal_lambda <- cv_model$lambda.min
print(paste("Optimal lambda:", optimal_lambda))

final_model <- glmnet(X, y, family = "binomial", alpha = 1, lambda = optimal_lambda)

coef(final_model)
summary(final_model)


```

For the final model selection, I conducted a Lasso Regression.

## Part 4 - Select Models

Now, we will select the most appropriate, fit, models that best represent our data.

### Selecting the most appropriate multiple regression model

To start, let us now evaluate the performances of the multiple regression models used to predict the cost of car crashes. To build on the fitness criteria that we already have available, I will derive AIC and BIC statistics:

```{r}
AIC_vals_lin <- AIC(lin_model_1, lin_model_2)
BIC_vals_lin <- BIC(lin_model_1, lin_model_2)

cat("Model 1 F-statistic:", summary(lin_model_1)$fstatistic[1], "\n")
cat("Model 2 F-statistic:", summary(lin_model_2)$fstatistic[1], "\n")
cat("Model 1 R^2:", summary(lin_model_1)$r.squared, "\n")
cat("Model 2 R^2:", summary(lin_model_2)$r.squared, "\n")
cat("Model 1 RSE:", summary(lin_model_1)$sigma, "\n")
cat("Model 2 RSE:", summary(lin_model_2)$sigma, "\n")


#summary(lin_model_2)
print(AIC_vals_lin)
print(BIC_vals_lin)

plot(lin_model_1)
plot(lin_model_2)



```
Both models Based on the comparisons among the Adjusted R^2 and residual standard error, lin_model_1--which contains more predictors--appears to be a better fit for our data. In addition, both the AIC and BIC statistics confirm this, as the values are lower for lin_model_1 compared to lin_model_2 (although it is important to mention the differences in complexity between the two models). Based on all of this evidence, it is evident that lin_model_1 is the most appropriate linear model for predicting the cost of car accidents. However, it is important to note that the Adjusted R^2 value is rather low (accounting for less than 1% of the variance in the model), and the RSE, AIC, & BIC values are relatively high. All of these indicators may suggest potential limitations regarding the predictive fit of this model, even if it is somewhat superior to lin_model_2.

In terms of the distribution of residuals, both unfortunately seem to deviate from normality, and display similar observations.

Finally, I will use an ANOVA to compare the two models:

```{r}
anova(lin_model_1, lin_model_2)
```
As we can see, the result is significant, suggesting that one model is a significantly better fit than the other, and based on the other statistics and criteria provided, it would seem that lin_model_1 is, indeed, a more appropriate fit.

Now, I will evaluate the accuracy of the model on our training dataset

```{r}
scored_target_AMT <- predict(lin_model_1, newdata = insur_train_clean_lin, type = "response")

insur_train_lin_pred <- insur_train_clean_lin %>% cbind(scored_target_AMT)

rmse <- sqrt(mean(na.omit(insur_train_lin_pred$scored_target_AMT - insur_train_lin_pred$TARGET_AMT)^2))
print(rmse)
```
Our RSME is rather large, potentially indicating that our model may be a poor fit for our training data. A general linear model, or otherwise, a more robust model may prove to be a better fit in the future.


### Selecting the most appropriate logistic regression model

Now, let's evaluate the fir of our logistic models

```{r}
cat("Null Deviance bin_model_1:", bin_model_1$null.deviance, "\n")
cat("Null Deviance bin_model_2:", bin_model_2$null.deviance, "\n")
cat("Null Deviance final_model:", final_model$nulldev, "\n")

cat("Residual Deviance bin_model_1:", bin_model_1$deviance, "\n")
cat("Residual Deviance bin_model_2:", bin_model_2$deviance, "\n")
cat("Residual Deviance final_model:", final_model$nulldev * (1 - final_model$dev.ratio), "\n")

#AIC & BIC Criteria
cat("AIC bin_model_1:", AIC(bin_model_1), "\n")
cat("AIC bin_model_2:", AIC(bin_model_2), "\n")
#Solution derived from StackOverflow: https://stackoverflow.com/questions/40920051/r-getting-aic-bic-likelihood-from-glmnet


tLL <- -deviance(final_model)
k <- final_model$df
n <- final_model$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)


BIC<-log(n)*k - tLL


cat("AIC final_model:", AICc, "\n")


cat("BIC bin_model_1:", BIC(bin_model_1), "\n")
cat("BIC bin_model_2:", BIC(bin_model_2), "\n")
cat("BIC final_model:", BIC, "\n")





```

Based on the criteria listed above, we can see that the Null deviances for all three models are the same, which is interesting, since these models used different predictors. Nevertheless, based on this alone, we could infer that all three models are as effective. However, we can see that the residual deviance, and AIC criteria are slightly lower for our first model, suggesting slightly better fit. However, it is notable that the BIC criteria for model 2 is lower than models 1 and 3, but this could be due to that model exhibiting less complexity compared to the other two models. 

In any case, it would appear that model 1 displays slightly better fit when compared to the two alternative models.

### Model Validation based on Training Data

```{r}
scored_target_bin <- predict(bin_model_1, newdata = insur_train_clean_bin, type = "response")
#Setting a threshold of .5
scored_target_bin <- ifelse(scored_target_bin > .5, 1, 0)

insur_train_bin_pred <- insur_train_clean_bin %>% cbind(scored_target_bin)
```

```{r}
confusion_matrix <- table(insur_train_bin_pred$scored_target_bin, insur_train_bin_pred$TARGET_FLAG)
confusion_matrix_with_totals <- addmargins(confusion_matrix)
print(kable(confusion_matrix_with_totals, format = "markdown"))
```
```{r}
confuse = confusionMatrix(as.factor(insur_train_bin_pred$scored_target_bin), as.factor(insur_train_bin_pred$TARGET_FLAG), positive = "1")

total <- sum(confuse$table)
error <- sum(confuse$table) - sum(diag(confuse$table))
error_rate <- error / total

plot(roc(insur_train_bin_pred$TARGET_FLAG, insur_train_bin_pred$scored_target_bin), main = "ROC Curve")

auc(insur_train_bin_pred$TARGET_FLAG, insur_train_bin_pred$scored_target_bin)

print(confuse$table)
print(glue("classification error rate: {error_rate}"))
print(glue("accuracy: {confuse$overall[['Accuracy']]}"))
print(glue("sensitivity: {confuse$byClass[['Sensitivity']]}"))
print(glue("specificity: {confuse$byClass[['Specificity']]}"))
print(glue("precision: {confuse$byClass[['Precision']]}"))
print(glue("F1: {confuse$byClass[['F1']]}"))
```
As we can see, this model seems to be a relatively apt fit for these data, with an accuracy rating of ~78%, and an error rate of ~22%. Furthermore, the specificity is very high (~99%), indicating an aptitude for correctly identifying true negatives.  That said, however, the precision is only around 64%, indicating the presence of false positive classifications. In addition, sensitivity is rather low at around ~35%, indicating a high false negative rate. Finally, the F1 score is only .45. This may be a result of the low sensitivity. In sum, perhaps these indicators suggest that this model is correctly able to identify true positive and negative cases, but it still does not completely capture all positive cases in the training dataset.

The final metric displayed is the Area under the Curve (AUC), which came out to ~.64. This metric also indicates that the model is adept at correctly predicting true positives and negatives.

In conclusion, these metrics do indicate that, overall, this model is an appropriate predictor of whether a car crash occurs; however, it does possess notable limitations in terms of having a high false negative rate.

### Predictions for the Evaluation Dataset

```{r}
TARGET_AMT <- predict(lin_model_1, newdata = insur_test_16, type = "response")
TARGET_FLAG <- predict(bin_model_1, newdata = insur_test_16, type = "response")

TARGET_FLAG <- ifelse(TARGET_FLAG > .5, 1, 0)

insur_test_final <- insur_test_16 %>% subset(select = -c(TARGET_AMT, TARGET_FLAG))
  
insur_test_final <- insur_test_final %>% cbind(TARGET_AMT, TARGET_FLAG)
```

